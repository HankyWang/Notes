# Notes on Data Mining #
----------
## 1. 导论 ##
### 定义： ###
数据挖掘是从大规模数据集中抽取隐含的有意义的规律或模式。
### 典型的数据挖掘应用： ###
1）客户细分；2）客户流失预测；3）客户价值分析；4）异常发现；5）个性化服务；6）数据库直销；7）改进工作效率；8）预警
### 数据挖掘的一般过程： ###
#### 1. 数据集的定义： ####
通常把进行分析的数据处理成一张宽表的形式，表的每一行称为一个**实例**或**对象**或**样本**，表的每一列称为**属性**或**特征**或**变量**。在有些数据集中，有一个特殊的属性，称为类标签。该属性指明了实例所属的类。类标签在进行分类或聚类等数据挖掘任务的时候会用到。*分类*时类标签时数据挖掘学习算法的指导，数据挖掘算法根据类标签学习各类的区分规则，从而对没有类标签的新的实例进行分类。*聚类*时，数据集的类标签初始为空，数据挖掘算法根据数据内在的规律给每个实例赋予合适的类标签值。
#### 2. 属性数据类型 ####
数据挖掘过程中，数据集的属性并不是整数、浮点、布尔、字符等基本类型。这是因为数据挖掘并不强调数据之间的精确计算，而是强调发现属性或属性数据之间的关系。计算时发现数据之间关系的重要手段。

#### **（1）离散型:** 离散性数据的特点时属性数据之间没有确定的顺序，不能进行常规的运算。 ####
- 名称型
- 类别型
- 序数型
#### **（2）连续型** 数据一般为数值，数据之间是有确定的顺序，可以互相比较和计算。####
- 范围型
- 比例型
#### 3. 数据挖掘的任务 ####
数据挖掘主要有两大类任务：**分类预测型任务**和**描述型任务**。
#### (1) 分类预测任务 ####
分类预测型任务从已知的已分类的数据中学习模型，并对**新的未知分类的数据使用该模型进行解释**，得到这些数据的分类。在有些文献中，根据类标签的不同，分别称之为**分类任务**与**预测任务**。如果类标签是**离散的类别**，则称为**分类任务** ；如果类标签是**连续的数值**，则称为**预测任务**。

----------
#### 几个数据集的概念 ####
**训练集**是数据挖掘中用来训练学习算法，建立模型的数据集。**测试集**是数据挖掘算法在生成模型后，用以测试所得到的模型的有效性的数据集。在实际操作中，所得模型效果不尽人意，还可以对训练集与测试集进行调整，重新进行训练。 **验证集**是在数据挖掘过程结束后，模型应用的实际数据集。验证集用于在实践中检验模型。

----------
#### (2) 描述型任务####
描述型任务根据给定数据集中数据内部的固有联系，生成对数据集中数据关系或整个数据集的概要描述。描述型任务主要包括**聚类**、**摘要**、**依赖分析**等子任务。聚类任务把没有预定义类别的数据划分为几个合理的类别，摘要任务形成数据集高度浓缩的子集及描述，依赖分析任务发现数据项之间的关系。
描述型任务是**直接在目标数据集上构造模型**，并得到模型处理的结果。随任务所要求的描述方式的不同，结果也可以按不同的方式进行展现。

#### 4. 数据挖掘过程 ####
数据挖掘不是一个简单的数据->模型->结果的简单的公式套用的过程，而是一个循环往复、精益求精的过程。主要由**业务理解**、**数据理解**、**数据预处理**、**建模**、**评估**、**部署**几个步骤组成。

![](http://www.thebigdata.cn/upload/2015-01/150128105064821.bmp)

#### （1）业务理解 ####
了解进行挖掘分析的业务，明确业务所要达到的目的和成功标准，估算该应用的资源和风险。将业务问题转换为数据挖掘的问题，确定数据挖掘的目标和成功标准，最后产生初步的项目计划。**注意**应用要达到的目的与数据挖掘的目的通常是不一样的。
#### （2）数据理解 ####
目的是对数据的本质与质量有所了解，如数据是否存在噪声、缺失值、冗余属性、不一致、数据过少或过多等问题。
#### （3）数据预处理 ####
准备最后的用于建模的数据集，包括数据选择、数据清理、数据合成、数据合并及数据格式化等子任务。
#### （4）建模 ####
首先，选择合适的建模算法并设置该算法的参数。通常对某一种挖掘任务，有多种数据挖掘的算法可以使用。但是各个算法对数据形式的要求可能不一样，因此可能需要回到上一步对数据重新进行处理。算法选定后即进行模型的训练。在训练过程中，调节算法的参数以达到最优的结果。训练完毕后，使用测试方案对模型测试。在训练的过程中，调节算法的参数已达到最优的结果。训练完毕后，使用测试方案对模型测试。
#### （5）评估 ####
对上述阶段得到的模型进行评估，课根据实际数据进行验证，还可以进行小规模的市场调研等。如果结果不理想，则应该返回以前的步骤甚至重新开始，否则进行下一阶段。
#### （6）部署 ####
将生成的结果进行适当的展示或嵌入一个业务过程中，并将模型应用的结果再反馈回来，以便对模型进行改进。对实际数据变化比较快、竞争激烈的环境中，如电信业、银行业等，还需要对模型经常进行维护。

各个步骤的实践并不是平均分配。大致来说，**业务理解：数据理解：数据预处理：建模：评估：部署所占实践比例大约为15%：5%：60%：5%：5%：10%**。其中，数据预处理一般话费整个数据挖掘过程中最多的时间，当然，各个步骤所占用的时间比例会有所变化。

### 数据挖掘的一般方法 ###
#### 1. 分类预测型方法 ####
#### （1）决策树方法 ####
决策树方法的原理是使用单个属性的超平面对输入的空间进行反复分割，从而将各个类别的实例区分开。先根据一个能够使原数据集中实例能够区分的最好的属性将数据集分割为多个子数据集。在子数据集的基础上，再进行划分。最后可以形成一个树状的划分结构。常见的算法有ID3/C4.5/C5.0、CART、CHAID等。

决策树算法**时间复杂度低**，可以处理**高维数据**，在大多数实际数据集中**分类精度**都令人满意。
#### （2）神经网络方法 ####
神经网络方法则是模仿生物神经元的刺激——反馈的学习方式对数据集进行的算法。有多种不同的神经网络，可用于不同的学习目的。常用的使用于分类的前馈神经网络。

前馈神经网络有多层组成，每层都具有若干个神经元，各层之间都进行链接。输入数据从第一层进入，经过各层神经元的变换与计算，最后各处输出。网络通过调整神经元使得**输入（实例的条件属性）**能够得到**正确输出（实例的类别）**。连接各处的过程使通过反馈进行的，即网络输出和正确输出的差被反馈回网络，从而不断对连接调整，直至输出正确。

神经网络的特点是能够获得**较高的分类准确度**，但是**参数调整较为复杂**，训练过程一般来说比较长。此外，神经网络结果的**可解释性比较差**，相当于一个黑盒。

#### （3）规则归纳方法 ####
规则归纳方法试图直接从数据集中形成间接的规则来描述数据集。规则并不一定百分百正确，只需要一定的支持率就可以了。与规则相悖时，并不妨碍规则的使用，只需要加上例外而已。

规则归纳的算法也有不少与决策树类似，一般的流程从单一的属性值开始，再逐渐称为一条完整的规则，再进行剪枝。一条规则相当于决策树中从根到叶子的一条分支，因而也可以将决策树转化为规则。具有代表性的规则归纳算法有AQ、C45rules、RIPPER。

规则归纳方法的特点时**结果易于理解和解释**，再**一些**数据集上可以取得**相当高的分类精度**。**有些方法的时间复杂度较低**，因而可以**应用于大规模数据集**。
#### （4）支持向量机 ####
支持向量机理论时根据结构风险最小化规则，尽量提高学习算法的泛化能力，即由有i按的训练集样本得到的晓的误差能够保证独立的测试集仍保持小的误差。支持向量机算法是一个凸优化的问题，因此局部最优解一定是全局最优解。而且算法的复杂度和实例集的维数无关。支持向量机算法根据输入样本计算该区域的决策曲面，由此确定该区域中未知样本的类别。

支持向量机的基本思想是通过某种事先选择的非线性映射（称为核函数）将输入向量映射到一个高维特征空间，在这个空间中构造最优线性分类超平面。在高维特征空间中构造最优线性分类超平面，需要计算映射后的向量的内积。由于核函数的特殊性质，只需要在院空间中计算核函数的值就可以了，从而克服了维度困难。通过选用不同的核函数，可以构造输入空间中不同类型的非线性决策面的学习机。


支持向量机在某些实际应用中表现较为突出，其时间复杂度也相对较小。
#### （5）贝叶斯方法 ####
贝叶斯方法的学习机制是利用贝叶斯公式将先验信息与样本信息综合，得到后验信息。在数据挖掘中，主要使用两种贝叶斯方法，即**朴素贝叶斯方法**与**贝叶斯网络**。

朴素bayes方法直接**利用贝叶斯公式进行预测**。把从预测样本中计算出的各个属性值和类别频率比作为先验概率，并假定各个属性之间是独立的，就可以用bayes公式和相应的概率公式计算出要预测实例的对各类别的条件概率值，然后可选区概率值最大的类别作为预测值。此方法简单易行且具有较好的精度。

贝叶斯网络是一个**带有概率注释的有向无环图**。这个图模型能有效地表示大的变量集合的联合概率分布，从而适合用来分析大量变量之间的互相关系，利用bayes公式的学习和推理功能，事先预测、分类等数据采掘任务。贝叶斯网络也是一种适合处理不确定性知识问题的方法，因为它可以从部分概率进行推导。构造贝叶斯网络需要进行网络结构和网络参数两部分的学习。为了建立贝叶斯网络，首先却低估问题的变量组，接着通过对变量之间的条件独立性的分析，建立一个表示这些变量之间的关系的有向无环图，最好需要对变量指派局部概率分布。在离散的情形下，需要为每一个变量的各个父节点的状态指派一个分布。**但是获得最优的结构和参数都是NP问题，因此存在许多用启发式方法进行学习的方法。**
#### （6）粗糙集方法 ####
粗糙集方法的出发点是知识的粒度性。数据表的每条数据都被看做是一个对象，对象的属性则描述了对象的一些特征。给定了一组特征，具有相同的这些特征的对象就组成了在该特征集合下的最小知识粒度。

对于分类问题，条件属性形成了一组知识，决策属性形成了另一组知识。分类问题转化为以条件属性组成的知识去表示决策属性组成的知识。不同的条件属性子集形成不同的知识粒度，通过找到最小的条件属性集合甚至是属性值集合，可以找到最佳分类规则。

粗糙集理论具有良好的数学性质和可解释性，但在应用与实际数据时，还要解决算法**时间复杂度高**、**数据中的噪声**等问题。
#### （7）回归分析 ####
回归分析试图用超平面去拟合实际数据。线性回归方法**假定条件属性和决策属性之间存在线性关系**，然后通过训练数据拟合出一个线性方程，并使用该方程去预测未知的新实例。因此，线性回归较适合近似线性的数据，也可以用来进行变量选择。

在线性回归的基础上，发展论logistic回归。该回归主要**处理二类问题**，与线性回归不同，其划分的曲线形式时S型，因而适应面更广。
#### （8）k-最近邻方法 ####
k-最近邻方法则将所有的实例都对应于n维空间中的点，那么一个新的未知点的最佳类别可以认为是由与该点距离最近的其他点（最近邻）来确定的。一个实例的最近邻一般根据欧氏距离定义。k-最近邻方法并不是从实例数据集中建立一个复杂的模型，而是在对新实例进行分类时计算给定实例在已有类标签的数据集中最接近的k各实例，并使用这k各实例的类别的投票来决策该实例的类别。k-最近邻方法是所谓的延迟学习方法，即把学习推迟到新的实例来临时再开始。虽然k-最近邻方法原理很简单，它假定一个实例的分类和在欧氏距离议一下于它邻近的实例的分类是相似的；但是k-最近邻方法还是有效的，它及其改进方法在数据挖掘中有广泛应用。

k-最近邻方法对k的个数比较敏感，需要经验进行确定。此外，简单投票法对数据中的噪声较为敏感，一个简单改进方法是对k个近邻的贡献值加权，越近的邻居具有越大的权值，使得较远的邻居有较小的影响。这一改进对于数据里的噪声具有很好的健壮性，可以消除孤立的噪声实例的影响。
####（9）组合学习方法 ####
组合学习方法并不是一个独立的方法，而是现有方法的组合。通过将多个分类方法以特殊的方法组合起来，分别对数据构造模型，然后对模型的输出进行综合，得出最终的分类决定。

有不同的组合分类器的方法，常见的有Bagging、Boosting、随机森林等。组合学习方在大多数情况下能得到**比单个方法分类器更好的结果**。但是其不利之处是**训练时间长**、**牺牲了部分模型的可解释性**。

#### 2. 描述型方法 ####
#### （1）聚类方法 ####
一般可以把数据挖掘算法分成有导师（或监督）和无导师学习两种方法，其主要区别是有没有将类信息作为指导。聚类是典型的无导师学习算法，一般用于自动分类。

聚类是按照某种热顶标准（通常是某种距离）把一个数据集分割成不同的类，使得类内相似性尽可能的大，同时类间的区别性也尽可能的大。直观的看，最终形成的每一个聚类，在空间商都是一个相对稠密的区域。

聚类方法主要包括**划分聚类**、**层次聚类**、**基于密度的聚类**和**Kohonen聚类**等。平面聚类方法通过优化一个评估函数把数据集分割成多个部分；分层聚类在不同层次上对数据进行分割，具有明显的层次性，算法执行过程可以用一棵层次树（多为二叉树）来进行描述；基于密度的聚类则考虑每个实例的邻域，根据实例的邻域来判断数据的密度变化，得到基于密度的划分。Kohonen是一种基于神经网络的聚类方法。

进行**分类聚类**，一般用距离度量对象之间的相似性，典型的为欧氏距离。距离越大，则相似性就越小，反之亦然。若用对象的平均值来表示聚类的中心，则可以用对象导中心的距离来作为分类的标准（判别函数），把对象分配到离他最近的一个聚类中去。因此，一般的基于距离的算法可以分为两个步骤：*首先*，将每个对象分配导距离最近的一个聚类或组中，*然后* ，重新调整该类的中心。反复重复这两个步骤，指导没有对象被重新分配，且满足判别函数（分类标准）为止。此类算法的很多典型如k-均值算法，它要求用户**给定聚类数目k值，然后任意选取k个点（数据）作为类中心开始迭代**，直到满足判别函数。


**层次聚类方法**可以*自底向上*和*自顶向下*进行。前者把实例看做单独的类，自下而上地按层次进行合并。后者则相反，先把整个实例集作为一个大的类，逐渐分裂。

此外，进来提出了一种概念聚类的方法，即概念格。该方法主要是从数据集中构造概念的层次关系，用于发现新的概念及探索概念之间的关系。此外，还能从格上提取关联规则和分类规则。

由于聚类是无导师的学习方法，其所研究的数据没有类别标签；因此很难判断得到的聚类划分是否反映了事物的本质。因此，对聚类的质量的评判都只是一些相对的标准，没有绝对的标准。
#### （2）关联分析 ####
关联分析的目的是找出数据集中属性值之间的联系，形成关联规则。关联规则带有两个参数，即支持率于置信度。支持率表示该关联规则在所有实例中成立的比例，也即规则所具有的代表性；置信度则说明了在规则前件成立的情况下，规则后件也成立的比例，也即规则所具有的可信度。

规则挖掘算法就是找到具有大于最小支持率和最小置信度的关联规则。一般可以分为两步进行，即先找到频繁项目集，即数据集中出现次数超过支持率的属性值集，再从中生成规则。目前大部分的算法集中于有效的寻找频繁项目集。

关联规则的典型算法是Apriori算法。该算法通过多次迭代找出所有频繁项目集，在第k次迭代过程中找出所有的频繁k项目集L<sub>k</sub>。该算法使用如下启发式规则：一个项目集是频繁项目集，则此项目集的所有子集构成的项目集也一定是频繁项目集；一个项目集是非频繁项目集，则此项目集的所有超集（即包含此项目集的项目集）一定是非频繁项目集。因此，第k次递归时的候选项目集可由第k-1次迭代时找出的所有频繁项目集之间通过连接运算得到。

**关联规则在实际应用中的问题时所产生的大量关联规则时无用的，即或者时人们早已熟知的关联，或者是实际意义很小的关联。要从中找到真正用户感兴趣的有用的关联规则则较为困难。**

#### 3. 文本/Web挖掘方法 ####
文本/Web挖掘的是非结构化的数据，其数据绝大多数是由自然语言来描述的。由于目前计算机自然语言理解的能力还处于较为初级的阶段，因此一将其转化为结构化数据进行处理，从而可以应用之前所讲的方法。

目前文档的表示大都是采用特征空间方法，文档被分解成一个个独立的词和词出现的频率，从中选取能够代表文档的词作为特征向量，每个文档由一个**特征向量**表示。一个有效的特征向量必须具备以下两个特征：**一是完全性，能体现目标特征；二是区分性，能将目标与其他文档区分开。**一般来说，词条重要性**正比于词条在文档中的频率，反比于训练文本中出现该词条的文档频率**。根据这条规律可使用这两个频率来确定词条的权重，这种方法称为*TFIDF模型*。

在确定了特征向量之后，对于信息检索来说。常用的方法由TFIDF、布尔检索、语义网络等，一般信息检索用精度（检索到的相关文档数/检索的文档数）、召回率（检索到相关文档数/相关文档数）来评价性能。

文本挖掘和Web挖掘的一部分是重叠的，都有文本检索、分类自动摘要等内容。这部分内容在Web挖掘中称为内容挖掘。Web挖掘的主要内容还包括结构挖掘与使用挖掘