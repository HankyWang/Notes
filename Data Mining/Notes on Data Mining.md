# Notes on Data Mining #

-------

## 第1章 导论 ##
### 定义： ###
数据挖掘是从大规模数据集中抽取隐含的有意义的规律或模式。
### 典型的数据挖掘应用： ###
1）客户细分；2）客户流失预测；3）客户价值分析；4）异常发现；5）个性化服务；6）数据库直销；7）改进工作效率；8）预警
### 数据挖掘的一般过程： ###
#### 1. 数据集的定义： ####
通常把进行分析的数据处理成一张宽表的形式，表的每一行称为一个**实例**或**对象**或**样本**，表的每一列称为**属性**或**特征**或**变量**。在有些数据集中，有一个特殊的属性，称为类标签。该属性指明了实例所属的类。类标签在进行分类或聚类等数据挖掘任务的时候会用到。*分类*时类标签时数据挖掘学习算法的指导，数据挖掘算法根据类标签学习各类的区分规则，从而对没有类标签的新的实例进行分类。*聚类*时，数据集的类标签初始为空，数据挖掘算法根据数据内在的规律给每个实例赋予合适的类标签值。
#### 2. 属性数据类型 ####
数据挖掘过程中，数据集的属性并不是整数、浮点、布尔、字符等基本类型。这是因为数据挖掘并不强调数据之间的精确计算，而是强调发现属性或属性数据之间的关系。计算时发现数据之间关系的重要手段。

#### **（1）离散型:** 离散性数据的特点时属性数据之间没有确定的顺序，不能进行常规的运算。 ####
- 名称型
- 类别型
- 序数型

#### **（2）连续型** 数据一般为数值，数据之间是有确定的顺序，可以互相比较和计算。####
- 范围型
- 比例型

#### 3. 数据挖掘的任务 ####
数据挖掘主要有两大类任务：**分类预测型任务**和**描述型任务**。
#### (1) 分类预测任务 ####
分类预测型任务从已知的已分类的数据中学习模型，并对**新的未知分类的数据使用该模型进行解释**，得到这些数据的分类。在有些文献中，根据类标签的不同，分别称之为**分类任务**与**预测任务**。如果类标签是**离散的类别**，则称为**分类任务** ；如果类标签是**连续的数值**，则称为**预测任务**。

----------
#### 几个数据集的概念 ####
**训练集**是数据挖掘中用来训练学习算法，建立模型的数据集。**测试集**是数据挖掘算法在生成模型后，用以测试所得到的模型的有效性的数据集。在实际操作中，所得模型效果不尽人意，还可以对训练集与测试集进行调整，重新进行训练。 **验证集**是在数据挖掘过程结束后，模型应用的实际数据集。验证集用于在实践中检验模型。

----------
#### (2) 描述型任务####
描述型任务根据给定数据集中数据内部的固有联系，生成对数据集中数据关系或整个数据集的概要描述。描述型任务主要包括**聚类**、**摘要**、**依赖分析**等子任务。聚类任务把没有预定义类别的数据划分为几个合理的类别，摘要任务形成数据集高度浓缩的子集及描述，依赖分析任务发现数据项之间的关系。
描述型任务是**直接在目标数据集上构造模型**，并得到模型处理的结果。随任务所要求的描述方式的不同，结果也可以按不同的方式进行展现。

#### 4. 数据挖掘过程 ####
数据挖掘不是一个简单的数据->模型->结果的简单的公式套用的过程，而是一个循环往复、精益求精的过程。主要由**业务理解**、**数据理解**、**数据预处理**、**建模**、**评估**、**部署**几个步骤组成。

<img src="http://www.thebigdata.cn/upload/2015-01/150128105064821.bmp">

#### （1）业务理解 ####
了解进行挖掘分析的业务，明确业务所要达到的目的和成功标准，估算该应用的资源和风险。将业务问题转换为数据挖掘的问题，确定数据挖掘的目标和成功标准，最后产生初步的项目计划。**注意**应用要达到的目的与数据挖掘的目的通常是不一样的。
#### （2）数据理解 ####
目的是对数据的本质与质量有所了解，如数据是否存在噪声、缺失值、冗余属性、不一致、数据过少或过多等问题。
#### （3）数据预处理 ####
准备最后的用于建模的数据集，包括数据选择、数据清理、数据合成、数据合并及数据格式化等子任务。
#### （4）建模 ####
首先，选择合适的建模算法并设置该算法的参数。通常对某一种挖掘任务，有多种数据挖掘的算法可以使用。但是各个算法对数据形式的要求可能不一样，因此可能需要回到上一步对数据重新进行处理。算法选定后即进行模型的训练。在训练过程中，调节算法的参数以达到最优的结果。训练完毕后，使用测试方案对模型测试。在训练的过程中，调节算法的参数已达到最优的结果。训练完毕后，使用测试方案对模型测试。
#### （5）评估 ####
对上述阶段得到的模型进行评估，课根据实际数据进行验证，还可以进行小规模的市场调研等。如果结果不理想，则应该返回以前的步骤甚至重新开始，否则进行下一阶段。
#### （6）部署 ####
将生成的结果进行适当的展示或嵌入一个业务过程中，并将模型应用的结果再反馈回来，以便对模型进行改进。对实际数据变化比较快、竞争激烈的环境中，如电信业、银行业等，还需要对模型经常进行维护。

各个步骤的实践并不是平均分配。大致来说，**业务理解：数据理解：数据预处理：建模：评估：部署所占实践比例大约为15%：5%：60%：5%：5%：10%**。其中，数据预处理一般话费整个数据挖掘过程中最多的时间，当然，各个步骤所占用的时间比例会有所变化。

### 数据挖掘的一般方法 ###
#### 1. 分类预测型方法 ####
#### （1）决策树方法 ####
决策树方法的原理是使用单个属性的超平面对输入的空间进行反复分割，从而将各个类别的实例区分开。先根据一个能够使原数据集中实例能够区分的最好的属性将数据集分割为多个子数据集。在子数据集的基础上，再进行划分。最后可以形成一个树状的划分结构。常见的算法有ID3/C4.5/C5.0、CART、CHAID等。

决策树算法**时间复杂度低**，可以处理**高维数据**，在大多数实际数据集中**分类精度**都令人满意。
#### （2）神经网络方法 ####
神经网络方法则是模仿生物神经元的刺激——反馈的学习方式对数据集进行的算法。有多种不同的神经网络，可用于不同的学习目的。常用的使用于分类的前馈神经网络。

前馈神经网络有多层组成，每层都具有若干个神经元，各层之间都进行链接。输入数据从第一层进入，经过各层神经元的变换与计算，最后各处输出。网络通过调整神经元使得**输入（实例的条件属性）**能够得到**正确输出（实例的类别）**。连接各处的过程使通过反馈进行的，即网络输出和正确输出的差被反馈回网络，从而不断对连接调整，直至输出正确。

神经网络的特点是能够获得**较高的分类准确度**，但是**参数调整较为复杂**，训练过程一般来说比较长。此外，神经网络结果的**可解释性比较差**，相当于一个黑盒。

#### （3）规则归纳方法 ####
规则归纳方法试图直接从数据集中形成间接的规则来描述数据集。规则并不一定百分百正确，只需要一定的支持率就可以了。与规则相悖时，并不妨碍规则的使用，只需要加上例外而已。

规则归纳的算法也有不少与决策树类似，一般的流程从单一的属性值开始，再逐渐称为一条完整的规则，再进行剪枝。一条规则相当于决策树中从根到叶子的一条分支，因而也可以将决策树转化为规则。具有代表性的规则归纳算法有AQ、C45rules、RIPPER。

规则归纳方法的特点时**结果易于理解和解释**，再**一些**数据集上可以取得**相当高的分类精度**。**有些方法的时间复杂度较低**，因而可以**应用于大规模数据集**。
#### （4）支持向量机 ####
支持向量机理论时根据结构风险最小化规则，尽量提高学习算法的泛化能力，即由有限的训练集样本得到的小的误差能够保证独立的测试集仍保持小的误差。支持向量机算法是一个凸优化的问题，因此局部最优解一定是全局最优解。而且算法的复杂度和实例集的维数无关。支持向量机算法根据输入样本计算该区域的决策曲面，由此确定该区域中未知样本的类别。

支持向量机的基本思想是通过某种事先选择的非线性映射（称为核函数）将输入向量映射到一个高维特征空间，在这个空间中构造最优线性分类超平面。在高维特征空间中构造最优线性分类超平面，需要计算映射后的向量的内积。由于核函数的特殊性质，只需要在院空间中计算核函数的值就可以了，从而克服了维度困难。通过选用不同的核函数，可以构造输入空间中不同类型的非线性决策面的学习机。


支持向量机在某些实际应用中表现较为突出，其时间复杂度也相对较小。
#### （5）贝叶斯方法 ####
贝叶斯方法的学习机制是利用贝叶斯公式将先验信息与样本信息综合，得到后验信息。在数据挖掘中，主要使用两种贝叶斯方法，即**朴素贝叶斯方法**与**贝叶斯网络**。

朴素bayes方法直接**利用贝叶斯公式进行预测**。把从预测样本中计算出的各个属性值和类别频率比作为先验概率，并假定各个属性之间是独立的，就可以用bayes公式和相应的概率公式计算出要预测实例的对各类别的条件概率值，然后可选区概率值最大的类别作为预测值。此方法简单易行且具有较好的精度。

贝叶斯网络是一个**带有概率注释的有向无环图**。这个图模型能有效地表示大的变量集合的联合概率分布，从而适合用来分析大量变量之间的互相关系，利用bayes公式的学习和推理功能，事先预测、分类等数据采掘任务。贝叶斯网络也是一种适合处理不确定性知识问题的方法，因为它可以从部分概率进行推导。构造贝叶斯网络需要进行网络结构和网络参数两部分的学习。为了建立贝叶斯网络，首先却低估问题的变量组，接着通过对变量之间的条件独立性的分析，建立一个表示这些变量之间的关系的有向无环图，最好需要对变量指派局部概率分布。在离散的情形下，需要为每一个变量的各个父节点的状态指派一个分布。**但是获得最优的结构和参数都是NP问题，因此存在许多用启发式方法进行学习的方法。**
#### （6）粗糙集方法 ####
粗糙集方法的出发点是知识的粒度性。数据表的每条数据都被看做是一个对象，对象的属性则描述了对象的一些特征。给定了一组特征，具有相同的这些特征的对象就组成了在该特征集合下的最小知识粒度。

对于分类问题，条件属性形成了一组知识，决策属性形成了另一组知识。分类问题转化为以条件属性组成的知识去表示决策属性组成的知识。不同的条件属性子集形成不同的知识粒度，通过找到最小的条件属性集合甚至是属性值集合，可以找到最佳分类规则。

粗糙集理论具有良好的数学性质和可解释性，但在应用与实际数据时，还要解决算法**时间复杂度高**、**数据中的噪声**等问题。
#### （7）回归分析 ####
回归分析试图用超平面去拟合实际数据。线性回归方法**假定条件属性和决策属性之间存在线性关系**，然后通过训练数据拟合出一个线性方程，并使用该方程去预测未知的新实例。因此，线性回归较适合近似线性的数据，也可以用来进行变量选择。

在线性回归的基础上，发展论logistic回归。该回归主要**处理二类问题**，与线性回归不同，其划分的曲线形式时S型，因而适应面更广。
#### （8）k-最近邻方法 ####
k-最近邻方法则将所有的实例都对应于n维空间中的点，那么一个新的未知点的最佳类别可以认为是由与该点距离最近的其他点（最近邻）来确定的。一个实例的最近邻一般根据欧氏距离定义。k-最近邻方法并不是从实例数据集中建立一个复杂的模型，而是在对新实例进行分类时计算给定实例在已有类标签的数据集中最接近的k各实例，并使用这k各实例的类别的投票来决策该实例的类别。k-最近邻方法是所谓的延迟学习方法，即把学习推迟到新的实例来临时再开始。虽然k-最近邻方法原理很简单，它假定一个实例的分类和在欧氏距离议一下于它邻近的实例的分类是相似的；但是k-最近邻方法还是有效的，它及其改进方法在数据挖掘中有广泛应用。

k-最近邻方法对k的个数比较敏感，需要经验进行确定。此外，简单投票法对数据中的噪声较为敏感，一个简单改进方法是对k个近邻的贡献值加权，越近的邻居具有越大的权值，使得较远的邻居有较小的影响。这一改进对于数据里的噪声具有很好的健壮性，可以消除孤立的噪声实例的影响。
####（9）组合学习方法 ####
组合学习方法并不是一个独立的方法，而是现有方法的组合。通过将多个分类方法以特殊的方法组合起来，分别对数据构造模型，然后对模型的输出进行综合，得出最终的分类决定。

有不同的组合分类器的方法，常见的有Bagging、Boosting、随机森林等。组合学习方在大多数情况下能得到**比单个方法分类器更好的结果**。但是其不利之处是**训练时间长**、**牺牲了部分模型的可解释性**。

#### 2. 描述型方法 ####
#### （1）聚类方法 ####
一般可以把数据挖掘算法分成有导师（或监督）和无导师学习两种方法，其主要区别是有没有将类信息作为指导。聚类是典型的无导师学习算法，一般用于自动分类。

聚类是按照某种热顶标准（通常是某种距离）把一个数据集分割成不同的类，使得类内相似性尽可能的大，同时类间的区别性也尽可能的大。直观的看，最终形成的每一个聚类，在空间商都是一个相对稠密的区域。

聚类方法主要包括**划分聚类**、**层次聚类**、**基于密度的聚类**和**Kohonen聚类**等。平面聚类方法通过优化一个评估函数把数据集分割成多个部分；分层聚类在不同层次上对数据进行分割，具有明显的层次性，算法执行过程可以用一棵层次树（多为二叉树）来进行描述；基于密度的聚类则考虑每个实例的邻域，根据实例的邻域来判断数据的密度变化，得到基于密度的划分。Kohonen是一种基于神经网络的聚类方法。

进行**分类聚类**，一般用距离度量对象之间的相似性，典型的为欧氏距离。距离越大，则相似性就越小，反之亦然。若用对象的平均值来表示聚类的中心，则可以用对象导中心的距离来作为分类的标准（判别函数），把对象分配到离他最近的一个聚类中去。因此，一般的基于距离的算法可以分为两个步骤：*首先*，将每个对象分配导距离最近的一个聚类或组中，*然后* ，重新调整该类的中心。反复重复这两个步骤，指导没有对象被重新分配，且满足判别函数（分类标准）为止。此类算法的很多典型如k-均值算法，它要求用户**给定聚类数目k值，然后任意选取k个点（数据）作为类中心开始迭代**，直到满足判别函数。


**层次聚类方法**可以*自底向上*和*自顶向下*进行。前者把实例看做单独的类，自下而上地按层次进行合并。后者则相反，先把整个实例集作为一个大的类，逐渐分裂。

此外，进来提出了一种概念聚类的方法，即概念格。该方法主要是从数据集中构造概念的层次关系，用于发现新的概念及探索概念之间的关系。此外，还能从格上提取关联规则和分类规则。

由于聚类是无导师的学习方法，其所研究的数据没有类别标签；因此很难判断得到的聚类划分是否反映了事物的本质。因此，对聚类的质量的评判都只是一些相对的标准，没有绝对的标准。
#### （2）关联分析 ####
关联分析的目的是找出数据集中属性值之间的联系，形成关联规则。关联规则带有两个参数，即支持率于置信度。支持率表示该关联规则在所有实例中成立的比例，也即规则所具有的代表性；置信度则说明了在规则前件成立的情况下，规则后件也成立的比例，也即规则所具有的可信度。

规则挖掘算法就是找到具有大于最小支持率和最小置信度的关联规则。一般可以分为两步进行，即先找到频繁项目集，即数据集中出现次数超过支持率的属性值集，再从中生成规则。目前大部分的算法集中于有效的寻找频繁项目集。

关联规则的典型算法是Apriori算法。该算法通过多次迭代找出所有频繁项目集，在第k次迭代过程中找出所有的频繁k项目集L<sub>k</sub>。该算法使用如下启发式规则：一个项目集是频繁项目集，则此项目集的所有子集构成的项目集也一定是频繁项目集；一个项目集是非频繁项目集，则此项目集的所有超集（即包含此项目集的项目集）一定是非频繁项目集。因此，第k次递归时的候选项目集可由第k-1次迭代时找出的所有频繁项目集之间通过连接运算得到。

**关联规则在实际应用中的问题时所产生的大量关联规则时无用的，即或者时人们早已熟知的关联，或者是实际意义很小的关联。要从中找到真正用户感兴趣的有用的关联规则则较为困难。**

#### 3. 文本/Web挖掘方法 ####
文本/Web挖掘的是非结构化的数据，其数据绝大多数是由自然语言来描述的。由于目前计算机自然语言理解的能力还处于较为初级的阶段，因此一将其转化为结构化数据进行处理，从而可以应用之前所讲的方法。

目前文档的表示大都是采用特征空间方法，文档被分解成一个个独立的词和词出现的频率，从中选取能够代表文档的词作为特征向量，每个文档由一个**特征向量**表示。一个有效的特征向量必须具备以下两个特征：**一是完全性，能体现目标特征；二是区分性，能将目标与其他文档区分开。**一般来说，词条重要性**正比于词条在文档中的频率，反比于训练文本中出现该词条的文档频率**。根据这条规律可使用这两个频率来确定词条的权重，这种方法称为*TFIDF模型*。

在确定了特征向量之后，对于信息检索来说。常用的方法由TFIDF、布尔检索、语义网络等，一般信息检索用精度（检索到的相关文档数/检索的文档数）、召回率（检索到相关文档数/相关文档数）来评价性能。

文本挖掘和Web挖掘的一部分是重叠的，都有文本检索、分类自动摘要等内容。这部分内容在Web挖掘中称为内容挖掘。Web挖掘的主要内容还包括结构挖掘与使用挖掘。结构挖掘从Web文档的结构信息中推导知识，不仅仅局限于文档之间的超链结构，还包括文档内部的结构、url中目录路径结构等。网络使用挖掘是从用户对网络的使用方式和行为中挖掘有用模式，用于对网络信息的合理组织和服务质量的改进，挖掘的结果通常是用户群体的共同行为和共性兴趣，以及个人信息的检索偏好、习惯和模式等。

通过网页之间的超链接的挖掘，可以生成网络站点的信息分类主题指南；通过记录用户在上一次上网过程中沿网页超链接漫游所浏览的URL路径，可以生成用户浏览的虚拟视图，便于用户在以后的上网的过程中加一利用；综合利用文本和超文本结构信息内容，对超文本进行分类，有可能取得更好的分类效果。通过指向每篇文章的链接的多少对文章的重要性进行评价，可用于检索结果文本的排序。通过分析网页之间的超链接关系和用户的存取模式，可对搜索引擎的信息分类与索引方式进行重新组织。

# 理论篇 #

----------
理论篇详细介绍了数据挖掘的算法，分别从分类方法、聚类方法、关联分析几个方面进行了介绍。包含的章节如下：

- 第2章 分类方法
- 第3章 聚类方法
- 第4章 关联分析
- 第5章 文本与Web挖掘

## 第2章 分类方法 ##
### 1. 决策树 ###
#### 决策树基本概念 ####

----------

决策树被广泛用作决策工具。在数据挖掘过程中，通过对数据的拟合、学习，自动地从训练数据中学习到一棵决策树。树的叶子结点代表了不同的决策类，非叶子结点代表了条件属性及其取值。从根节点到叶子结点的路径就是决策的过程，也可以用一条决策规则表示。

决策树的特点是**非常直观，易于理解，符合人们的决策思维**。此外，决策树也很**容易转化为规则的形式**。本质思想是使用超平面**对数据进行递归划分**。

决策树通过属性值形成的超平面对数据集进行反复切割，直到能把每一个决策类分开为止，切割的过程形成了一棵决策树。决策树的每个叶子结点**所支持的实例个数**，是这个叶子结点的**支持度**。支持度越大，说明在数据集中能够支持从根到叶子的这条规则的实例越多，该规则越有力。反之，如果该支持度越小，说明规则越弱。有力的规则往往具有较强的预测能力。

决策树不仅可以应用与离散的属性值，也可以对连续的属性值进行划分。其方法为找到连续属性中的若干个分点，使用这些分点将属性划分成若干个区间。，每个区间看做一个新的属性值，此过程称为属性离散化。

#### 决策树的构造过程 ####
----------
由于决策树是由属性值划分数据集所形成的，那么就能分解成两个问题。

（1） 选择哪一个属性进行第一次划分，接着使用哪一个属性进行划分？采用任意属性都能对数据集进行划分；但最后形成的决策树会差异很大，有的是非常简化的，有的则非常臃肿。对于分类算法来说，简洁的表示泛化性好，即对未知的实例分类效果更好。因此，需要寻找最优的属性选择方法。
>定义由属性进行划分的度量，根据该度量可以计算出对当前数据子集来说最佳的划分属性。常见的度量方法有信息增益（ID3）及其改进增益率（C4.5）、Gini系数（CART）、卡方检验（CHAID）等。

（2） 什么时候应该停止继续划分？理论上，划分可以直到数据子集都属于同一个类别为止，但是这样得到的决策树很可能层次太深，甚至每一个叶子结点都只有一个实例。这样的叶子结点支持度不够，预测能力弱。因而需要有一个条件来控制提前停止划分。
>对于问题（2）有几个处理的方法：一种是定义一个停止树进一步生长的条件；另一种是在生成完全的树之后再进行剪枝。

#### 1. 选择属性进行划分 ####
**信息增益方法**基于信息熵原理。信息熵是对信息混乱程度的一个度量。一般来说，信息如果是均匀的混合分布，则信息熵就高。若信息呈一致性分布，信息熵就低。在决策树中，类标签表示信息，即若数据子集类别混合均匀分布，则信息熵较高。若类别单一分布，则信息熵较低。**通过比较每个属性形成划分的前后的信息熵变化，选择信息熵朝最小的方向变化的属性，就能使得决策树最快达到叶节点，从而构造紧凑的决策树**。具体来说，对每个数据集/数据子集，信息熵可以定义为：


<img src="http://chart.googleapis.com/chart?cht=tx&chl=ENT(D_j)=-\sum\limit_{i=0}^{c-1}p_ilog_2p_i," style="display:block;margin-left:auto;margin-right:auto">

式中，c是数据集/子集D<sub>j</sub>中决策类的个数，p<sub>i</sub>是第i个决策类在D中的比例。

某属性的信息增益为未进行划分时数据集的信息熵与划分后数据子集的信息熵加权和的差，即：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=GAIN(A)=ENT(D)-\sum\limit_{j=1}^k\frac{|D_j|}{|D|}ENT(D_j)," style="display:block;margin-left:auto;margin-right:auto">

式中，A是候选属性，k是该属性的分指数；D是未使用A进行划分的数据集，D<sub>j</sub>是由A划分而成的子数据集；|D|代表数据集的实例个数。**在所有属性中，具有最大GAIN（A）的属性被选为当前进行划分的结点。**


----------


**Gini系数**从另一方面刻画了**信息的纯度**。该方法用于计算从相同的总体中随机选择的两个样本来自与不同类别的概率，即：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=Gini(D_j)=1-\sum\limit_{i=0}^{c-1}{p_i}^2," style="display:block;margin-left:auto;margin-right:auto">

对于任一个属性，将数据集划分为多个数据子集，则未进行划分时的数据集的Gini系数与划分后数据子集的Gini系数加权和的差为

<img src="http://chart.googleapis.com/chart?cht=tx&chl=G(A)=Gini(D)-\sum\limit_{j=1}^k\frac{|D_j|}{|D|}Gini(D_j)," style="display:block;margin-left:auto;margin-right:auto">

**在所有属性中，具有最大G(A)的属性选为当前进行划分的结点。**


----------
上述两者的计算结果时类似的。但是，信息增益方法与Gini系数法都存在一个问题，即它们偏向于具有很多不同值的属性，因为多个分支能降低信息熵或者Gini系数。决策树划分为很多分支会降低决策树的适用性。为解决此问题，在CART算法中进行了限制，即决策点只能由两个划分，因此CART只能生成二叉树。为生成二叉树，CART使用Gini系数测试属性值的两两组合，找出最好的二分方法。而C4.5中，则采用了信息增益的改进形式——**增益率**来解决该问题。**增益率**在信息增益中引入了该结点的分支信息，对分支过多的情况进行了惩罚，即：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=GAIN'RATIO(A)=\frac{GAIN(A)}{-\sum\limit_{j=1}^{k}\frac{|D_j|}{|D|}log_2\frac{|D_j|}{|D|}}," style="display:block;margin-left:auto;margin-right:auto">

式中，k是属性A的分支数，|D|代表数据集的实例个数。当k变大时，分母趋向于取较大的值，从而使得增益率变小。

------
**CHAID方法**则使用卡方检验寻找最优的划分属性。对于连续属性，它使用卡方检验方法将其离散化（参见7.5.3节）。对于离散属性，使用卡方检验法查找可以合并的值。卡方检验时统计学上广泛使用的样本间差异的检验方法。**较高的卡方值说明样本的差异显著。** 在各个属性使用卡方检验合并了区间之后，对每个属性计算其p-value值，**选择p-value最小的属性作为当前划分的属性。**

#### 2. 获得大小合适的树 ####
决策树学习的目的时获得简洁而预测能力强的树。在树完全生长的时候有可能其预测能力反而会降低。为解决此问题，需要获得大小合适的树。一般来说，有两种方法。

##### 1. **定义树停止生长的条件**。常见的条件如下： 
- *最小划分实例数*：当当前结点对应的数据子集的大小下雨制定的最小划分实例数时，即是它们不属于同一类，也不再进行进一步划分。
- *划分阈值*：当使用的划分方法所得到的值于父节点的值差小于指定阈值时不再划分。
- *最大树深度*：当进一步的划分时将超过最大树深度的时候，停止划分。

##### 2.生成完全的决策树后进行剪枝。 #####
通过对决策树的子树进行评估，若去掉某个子树后整个决策树表现更好，则该子集被剪枝。具体来说，在C4.5和CART中有不同的做法。

在CART中，对每个子树构造了一个成本复杂性指数，该指数综合考虑了误分类错误率和树的结构复杂度。然后从完全树开始，构造了一系列结构越来越简化的树，从中选择该指数最小的树作为最好的树。其中，误分类率要采用一个单独的剪枝集(不同于训练集的数据集的一个子集)来评估。树的结构复杂度则由树的终端结点个数与每个终端结点的成本的积来刻画。

C4.5则采用了悲观剪枝法。它不使用剪枝集去估计树的误分类率，而是用树的叶节点在训练集上的错误率，在一定的置信度上根据二项式分布估计其在未知数据上的错误率上限。根据该值来确定是否进行剪枝。C4.5的剪枝的过程中，可以将子树完全剪掉，也可以用该子树的某一个子树来代替。

#### 3.决策树的拓展 ####
1. 多元划分：为进一步提高效率，在每一个结点采用几个属性的线性组合来进行划分，相当于采用倾斜的决策面进行划分。然而，找到最优的线性属性组合非常困难，只能采用一些启发式的方法，如爬山法（在CART中使用）。采用多元属性进行划分的决策树的性能要好于传统的决策树，计算复杂度则远高于一元决策树。
2. 超大数据集：传统的决策树算法需要所有数据集均在内存中，对于超大规模的数据集来说这不实际。解决超大数据集的问题的一个方法就是采样。另外一个方面则可以对传统决策树进行改进，如SLIQ算法。SLIQ算法采用与排序方法，使得对于数值属性，只需要在开始排序一次，而无需在每个内部结点都重新排序；树的生长采用广度优先方法；对于离散属性，则只选择属性值的一部分进行评估。SLIQ在决策树的每个层次只需要最多两次数据扫描，因而能够处理驻留在磁盘上的数据集。
3. 成本矩阵：使用成本矩阵可指定将一个类错误划分到另一类的成本。对于支持成本举证的决策树算法，可以给异常数据分类为正常数据指定到更大的误分类成本值，强迫决策树算法更关注异常数据，从而在异常数据类上的性能得到提升，满足应用的需求。决策树可通过寻找最优划分点的计算方法进行拓展而支持成本矩阵。

----------

----------

### 2. 前馈神经网络 ###
#### 基本概念 ####
神经元根据起输入做出响应（由激励函数确定），链接的强度（权重）随着输入不断进行调整，直至对所有输入都响应正确为止。前馈神经网络是一种分层次的人工神经网络，主要用于分类与预测。

一个典型的前馈神经网络由输入层、输出层和若干个中间层（又称为隐层，因其对输入输出不可见）组成，每层由若干个神经元组成，层间的结点为全连接，层内的结点无连接。一般地，输入层和输出层神经元的个数由training set确定。各层结点之间的连接是有权重的，每个结点的输入由连接到它的各个结点的输出的加权和确定（输入层除外），**如h<sub>mn</sub>的输入为**:

<img src="http://chart.googleapis.com/chart?cht=tx&chl={\sum\limit_{k=1}^{p}O_{i_k}W_{i_kh_{mn}}}%2B{b_{h_{mn}}}" style="display:block;margin-left:auto;margin-right:auto">

式中，O<sub>i<sub>k</sub></sub>为结点i<sub>k</sub>的输出，W<sub>i<sub>k</sub>h<sub>mn</sub></sub>为结点i<sub>k</sub>到结点h<sub>mn</sub>的权重，b<sub>h<sub>mn</sub></sub>为该结点的偏置（用于克服输入全为零时训练无法进行的情况）。

偏置在神经元网络中常常表示呈隐层或输出层的一个单独输入（其输入是1，偏置量表现为权重），可将上式改写为：

<img src="http://chart.googleapis.com/chart?cht=tx&chl={\sum\limit_{k=0}^{p}O_{i_k}W_{i_kh_{mn}}},(k=0,O_{i_0}=1,W_{i_0h_{mn}}=b_{h_{mn}})" style="display:block;margin-left:auto;margin-right:auto">

神经网络的**输出**由它的激励函数所确定。常用的激励函数由Sigmod函数。该函数表现为两种形式，一种是logistic函数，即

<img src="http://chart.googleapis.com/chart?cht=tx&chl=f(x)=\frac{1}{1%2Be^{-ax}," style="display:block;margin-left:auto;margin-right:auto">

式中，a>0是一个常数，可以控制曲线的斜率，图形如下：

<img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png">函数的自变量取值范围为(0,1)

另一种是双曲正切（tanh）函数，即

<img src="http://chart.googleapis.com/chart?cht=tx&chl=f(x)=\frac{1-e^{-ax}}{1%2Be^{ax}}," style="display:block;margin:auto">
式中，α，β>0是常数，可以控制曲线的斜率，它与Sigmod函数的图形类似，但取值范围是(-1,1)或(-β,β)。

此外，还有比较少用的阈值函数，即：

<img src="http://latex.codecogs.com/svg.latex?f(x)=\left\{\begin{aligned}1,x\geq0;\\2,x<0.\end{aligned}\right" style="display:block;margin:auto">

从输入到输入层即从输出到输出层都是直接连接。具有一个隐层的前馈神经网络可以近似任何连续函数。在实际应用中，最常用的也是三层网络。出于网络复杂性、训练难度、防止过拟合等原因，也可以使用三层以上的网络。


----------
#### BP训练过程 ####
##### 1.算法描述 #####
如果网络的结构（即隐层数量），每层的结点数已经确定，则前馈神经网络的训练过程就是确定各结点之间的连接的权重。最常用的训练方法是反向传播算法，即BP（Back-Propagation）算法。该算法根据训练样本在网络正向传播时产生的误差，从输出结点反向逐层修正每个连接的权重，从而使得网络对输入收敛。

前馈网络的BP训练过程可简述如下：
>算法2-1 BP网络训练过程

>根据训练集确定网络结构；

>初始化网络权重为随机小数；

>初始化总误差为大于预先设定阈值。

>设定最大迭代次数；

>迭代次数=1；

>若总误差大于预先设定的阈值或者迭代次数小于设定次数，重复下列过程：

>>总误差=0；

>>对训练集的每个样本:

>>>从输入层开始，逐层计算每层网络的输出，直至输出层；

>>>计算输出层的输出和期望输出的差的平方和E；

>>>总误差=总误差+E；

>>>根据E反向调整各层的网络权重；

>>总误差 = 总误差/2

>>迭代次数 = 迭代次数+1

>结束

##### 2.相关公式 #####
假定用y<sub>i</sub>(n)代表第n次前向迭代中结点i的输出，v<sub>j</sub>(n)代表第n次前向迭代中结点j的输入，则由
<img src="http://chart.googleapis.com/chart?cht=tx&chl=v_j(n)=\sum\limit_{i=0}^rw_{ji}(n)y_i(n)," style="margin:auto;display:block">

<img src="http://chart.googleapis.com/chart?cht=tx&chl=y_j(n)=f(v_j(n))," style="display:block;margin:auto">

式中，r是所有连接到结点j的结点；w<sub>ji</sub>是这些结点的权重；i=0时，w<sub>ji</sub>即是结点的偏置。

如果j是输出结点，假定d<sub>j</sub>(n)代表结点j的正确的输出（按照训练集），则结点j输出的偏差为：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=e_j(n)=d_j(n)-y_j(n)," style="margin:auto;display:block">

可定义网络的输出误差为所有输出结点的偏差的平方和，即

<img src="http://chart.googleapis.com/chart?cht=tx&chl=\varepsilon(n)=\frac12\sum\limit_j{e_j}^2(n)." style="margin:auto;display:block">

则网络训练的目的就最小化上式即权重的函数。计算可得到网络中权重的调节公式为:

<img src="http://chart.googleapis.com/chart?cht=tx&chl=\Delta w_{ji}(n)=-\eta\frac{\partial\varepsilon(n)}{\partial w_{ji}(n)}=\eta\delta_j(n)y_i(n)" style="margin:auto;display:block">

式中，<img src="http://chart.googleapis.com/chart?cht=tx&chl=\eta">是学习速率， <img src="http://chart.googleapis.com/chart?cht=tx&chl=\delta_j(n)"> 称为局部梯度。

故某条连接的权重变化由该连接的入节点的局部梯度与该连接的出节点的输出的乘积所决定，使用学习速率可调节梯度下降的速度。

对于输出层结点j，局部梯度表示为输出误差与该结点输出的微分的积，即
<img src="http://chart.googleapis.com/chart?cht=tx&chl=\delta_j(n) = e_j(n)f^'_j(v_j(n)) ." style="margin:auto;display:block">

对于隐层结点，局部梯度的值依赖于它的上一层结点（输出层或上一个隐层），表现为该结点输出的微分与上一层结点局部梯度加权和的积，即

<img src="http://chart.googleapis.com/chart?cht=tx&chl=\delta_j(n)=f^'_j(v_j(n))\sum\limit_{k=1}^c\delta_k(n)w_{kj}(n)" style="margin:auto;display:block">

对于logistics函数，其微分可表示为

<img src="http://chart.googleapis.com/chart?cht=tx&chl=f^'_j(v_j(n))=\frac{\al e^{-\al v_j(n)}}{(1%2Be^{-\al v_j(n)})^2}=\al y_i(n)(1-y_i(n))" style="margin:auto;display:block">


对于双曲正弦函数，其微分可表示为

<img src="http://chart.googleapis.com/chart?cht=tx&chl=f^'_j(v_j(n))=\frac\al\be (\be-y_i(n))(\be%2By_i(n))" style="margin:auto;display:block">

虽然<img src="http://chart.googleapis.com/chart?cht=tx&chl=\eta">可调节学习速率，但是当<img src="http://chart.googleapis.com/chart?cht=tx&chl=\eta">过大是，容易使网络变得不稳定；而当<img src="http://chart.googleapis.com/chart?cht=tx&chl=\eta">过小时，则容易使得网络学习速率过慢。为使得学习速率较快而避免网络不稳定，可在权重调节公式加入一个动量项：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=\Delta w_{ji}(n)=\al\Delta w_{ji}(n-1)+\eta\delta_j(n)y_i(n)" style="margin:auto;display:block">

式中，<img src="http://chart.googleapis.com/chart?cht=tx&chl=\al">>0，称为动量常数。



